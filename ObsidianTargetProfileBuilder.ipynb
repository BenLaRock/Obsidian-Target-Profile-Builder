{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8e06a3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pptx import Presentation\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import markdown\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1db31e6",
   "metadata": {},
   "source": [
    "### Read in Obsidian files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "e61debeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProfileBuilder():\n",
    "    def __init__(self, entities_path, assets_path):\n",
    "        print(\"init()\")\n",
    "        self.entities_to_parse = []\n",
    "        self.raw_entities_data = {}\n",
    "        self.final_entities_data = {}\n",
    "        self.PERSON_ATTRIBUTES = (\n",
    "            (\"Detail\", \"details\"),\n",
    "            (\"Note\", \"notes\"),\n",
    "            (\"Associate\", \"associates\"),\n",
    "            (\"Email Address\", \"email_addresses\"),\n",
    "            (\"Phone Number\", \"phone_numbers\"),\n",
    "            (\"Residence Name\", \"residence_names\"),\n",
    "            (\"Residence Address\", \"residence_addresses\"),\n",
    "            (\"Residence Coordinates\", \"residence_coordinates\"),\n",
    "            (\"Residence Map\", \"residence_maps\"),\n",
    "            (\"Work Name\", \"work_names\"),\n",
    "            (\"Work Address\", \"work_addresses\"),\n",
    "            (\"Work Coordinates\", \"work_coordinates\"),\n",
    "            (\"Work Map\", \"work_maps\"),\n",
    "        )\n",
    "        \n",
    "        self.entities_path = entities_path\n",
    "        self.assets_path = assets_path\n",
    "        # self.create_profiles()\n",
    "\n",
    "    def create_profiles(self):\n",
    "        print(\"create_profiles()\")        \n",
    "        self.scan_dir_for_markdown_files()\n",
    "        print(\"entities_to_parse\", self.entities_to_parse)\n",
    "        \n",
    "        self.turn_markdowns_into_soups()\n",
    "        print(\"# of valid entities\", len(self.raw_entities_data))\n",
    "        \n",
    "        return self.parse_info_from_soups()\n",
    "    \n",
    "    def scan_dir_for_markdown_files(self):\n",
    "        for file in os.listdir(self.entities_path):\n",
    "            if file.endswith(\"md\"):\n",
    "                self.entities_to_parse.append(file)\n",
    "    \n",
    "    def turn_markdowns_into_soups(self):\n",
    "        for i in self.entities_to_parse[0:]:\n",
    "            print(\"i\", i)\n",
    "            filename = i.replace(\".md\", \"\")\n",
    "            \n",
    "            html = None\n",
    "            with open(f\"{self.entities_path}/{i}\", \"r\", encoding=\"utf-8\") as f:\n",
    "                text = f.read()\n",
    "            html = markdown.markdown(text)\n",
    "            soup = bs(html)\n",
    "            \n",
    "            if not self.is_file_person_of_interest(filename, soup):\n",
    "                continue\n",
    "            \n",
    "            # Maybe later combine these 2 dicts into one and delete the unneeded values before output\n",
    "            self.raw_entities_data[filename] = {\"soup\": soup}\n",
    "            # self.final_entities_data[filename] = {}\n",
    "\n",
    "    def is_file_person_of_interest(self, filename, soup):\n",
    "        all_p_tags = soup.find_all(\"p\")        \n",
    "        if any(\"person_of_interest\" in tag.get_text().lower() for tag in all_p_tags):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    def parse_info_from_soups(self):\n",
    "        for name, data in self.raw_entities_data.items():\n",
    "            self.parse_person_info(name, data)\n",
    "        \n",
    "        return [{name: data[\"parsed\"]} for name, data \n",
    "                in self.raw_entities_data.items()]\n",
    "    \n",
    "    def parse_person_info(self, name, data):\n",
    "        data[\"parsed\"] = {}\n",
    "        \n",
    "        # Get all paragraph and list item tags\n",
    "        all_p_and_li_tags = []\n",
    "        all_p_and_li_tags += data[\"soup\"].find_all(\"p\")\n",
    "        all_p_and_li_tags += data[\"soup\"].find_all(\"li\")\n",
    "        # print(all_p_and_li_tags)\n",
    "        \n",
    "        # Find pic filename and store pic path\n",
    "        pic_path = self.get_person_pic_path(all_p_and_li_tags)\n",
    "        data[\"parsed\"].update({\"entity_pic_path\": pic_path})\n",
    "        \n",
    "        # Parse all the other wanted information from the file\n",
    "        wanted_info = self.find_tags_matching_attributes(all_p_and_li_tags)\n",
    "        data[\"parsed\"].update({\"entity_info\": wanted_info})\n",
    "\n",
    "    def get_person_pic_path(self, tags):\n",
    "        person_pic = None\n",
    "\n",
    "        # Find correct p tag to parse\n",
    "        try:\n",
    "            person_pic = list(filter(lambda x: \"picture\" in x.get_text().lower(), tags))[0]\n",
    "        except IndexError:\n",
    "            print(\"There is no p tag or li tag with a picture in it\")\n",
    "            pass\n",
    "\n",
    "        if not person_pic:\n",
    "            return\n",
    "\n",
    "        # Parse filename from p tag\n",
    "        bracketed_filename_pattern = \"([0-9A-Z]{1,}[\\_\\s]{0,}[0-9A-Z]{1,}\\.{1}[0-9A-Z]{1,3})\"\n",
    "        try:\n",
    "            person_pic = re.search(bracketed_filename_pattern, str(person_pic), re.IGNORECASE)[0]\n",
    "        except TypeError:\n",
    "            print(\"There is no pic filename\")\n",
    "            person_pic = None\n",
    "            pass\n",
    "        \n",
    "        if not person_pic:\n",
    "            return\n",
    "        \n",
    "        return f\"{self.assets_path}/{person_pic.strip()}\"\n",
    "    \n",
    "    def find_tags_matching_attributes(self, tags):\n",
    "        info = {}\n",
    "        \n",
    "        for attr in self.PERSON_ATTRIBUTES:\n",
    "#             print(\"attr: \", attr)\n",
    "            info[attr[1]] = []\n",
    "            \n",
    "            matching_tags = list(filter(lambda x: attr[0] in x.get_text(), tags))\n",
    "#             print(\"matching tags: \", matching_tags)\n",
    "            \n",
    "            cleaned_texts = self.extract_info_from_matching_tags(matching_tags)\n",
    "            info[attr[1]] += cleaned_texts\n",
    "        \n",
    "        return info\n",
    "    \n",
    "    def extract_info_from_matching_tags(self, tags):\n",
    "#         print(f\"There are {len(tags)} matching tags here\")\n",
    "        tags = self.remove_code_tag_substring(tags)\n",
    "        tags = self.get_text_inside_tags(tags)\n",
    "        tags = self.extract_bracketed_link_texts(tags)\n",
    "        return tags\n",
    "        \n",
    "    def remove_code_tag_substring(self, tags):\n",
    "        cleaned_tags = []\n",
    "        for tag in tags:\n",
    "            cleaned_tag = None\n",
    "            try:\n",
    "                cleaned_tag = re.sub(\"<code>.*</code>\", \"\", str(tag))\n",
    "            except:\n",
    "                # print(\"substring doesn't have a `Code` tag in it\")\n",
    "                pass\n",
    "            if cleaned_tag:\n",
    "                cleaned_tags.append(cleaned_tag)\n",
    "        return cleaned_tags\n",
    "\n",
    "    def get_text_inside_tags(self, tags):\n",
    "        inside_texts = []\n",
    "        for tag in tags:            \n",
    "            inside_text = None\n",
    "            # Check for text inside p tags\n",
    "            try:\n",
    "                inside_text = re.match(\"(<p>)(.*)(</p>)\", tag)[2]\n",
    "            except TypeError:\n",
    "                # print(\"No text inside p tags\")\n",
    "                pass\n",
    "            # Check for text inside li tags\n",
    "            try:\n",
    "                inside_text = re.match(\"(<li>)(.*)(</li>)\", tag)[2]\n",
    "            except TypeError:\n",
    "                # print(\"No text inside li tags\")\n",
    "                pass\n",
    "            if inside_text:\n",
    "                inside_texts.append(inside_text.strip())\n",
    "        return inside_texts\n",
    "\n",
    "    def extract_bracketed_link_texts(self, texts):\n",
    "        cleaned_texts = []\n",
    "        for text in texts:\n",
    "            cleaned_text = None\n",
    "            try:\n",
    "                match_obj = re.match(\"(.*)(\\[\\[)(.*)(\\]\\])(.*)\", text)        \n",
    "        #         print(\"0\", match_obj[0]) # match object\n",
    "        #         print(\"1\", match_obj[1]) # match group 1, etc...\n",
    "        #         print(\"2\", match_obj[2])\n",
    "        #         print(\"3\", match_obj[3])\n",
    "        #         print(\"4\", match_obj[4])\n",
    "        #         print(\"5\", match_obj[5])\n",
    "                text_before = match_obj[1]\n",
    "                text_inside = match_obj[3]\n",
    "                text_after = match_obj[5]\n",
    "                rebuilt = f\"{text_before.strip()} {text_inside.strip()} {text_after.strip()}\"\n",
    "                cleaned_text = rebuilt.strip()\n",
    "            except TypeError:\n",
    "                # print(\"There was no linked text in double brackets\")\n",
    "                cleaned_text = text.strip()\n",
    "            if cleaned_text:\n",
    "                cleaned_texts.append(cleaned_text)\n",
    "        return cleaned_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "5296b850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init()\n",
      "create_profiles()\n",
      "entities_to_parse ['Arron Gaines.md', 'Elsie Graves.md', 'Jan Jackson.md', 'Kevin Ellison.md', 'Wilfredo Goodman.md']\n",
      "i Arron Gaines.md\n",
      "i Elsie Graves.md\n",
      "i Jan Jackson.md\n",
      "i Kevin Ellison.md\n",
      "i Wilfredo Goodman.md\n",
      "# of valid entities 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'Arron Gaines': {'entity_pic_path': 'Investigation/Assets/Arron_Gaines.jpg',\n",
       "   'entity_info': {'details': ['Subject likely lives and works in New York City, New York based on social media including pictures and check-ins. Follower/following relationships on social media platforms indicate a connection with Austin, Texas-based Kevin Ellison . Research on social engineering forums revealed usernames that were traced back to email addresses with a common web domain which has been separately flagged for malicious activity. Additional research on the emails suggests Gaines and Ellison are the probable end users.'],\n",
       "    'notes': ['No previous criminal convictions',\n",
       "     'Known phishing victims include: [[Elsie Graves]], [[Wilfredo Goodman]], and Jan Jackson'],\n",
       "    'associates': ['Kevin Ellison'],\n",
       "    'email_addresses': ['<em>probably uses</em> mglanman@mycbt(.)me'],\n",
       "    'phone_numbers': ['+1472-217-7096', '+1(202)-918-2132'],\n",
       "    'residence_names': ['350 Central Park West Apartments'],\n",
       "    'residence_addresses': ['350 Central Park West, New York, NY 10025'],\n",
       "    'residence_coordinates': ['40.79078819123121, -73.96568568759587'],\n",
       "    'residence_maps': ['<br/> ! 350_Central_Park_West_Apartments.JPG | 500'],\n",
       "    'work_names': ['undetermined organization at 30 Rockefeller Plaza'],\n",
       "    'work_addresses': ['30 Rockefeller Plaza, New York, NY 10112'],\n",
       "    'work_coordinates': ['40.758941635700246, -73.97900684718394'],\n",
       "    'work_maps': ['<br/> ! 30_Rockefeller_Plaza.JPG | 500']}}},\n",
       " {'Kevin Ellison': {'entity_pic_path': 'Investigation/Assets/Kevin_Ellison.jpg',\n",
       "   'entity_info': {'details': ['Subject likely lives and works in the Austin, Texas area based on social media including pictures and check-ins. Follower/following relationships on social media platforms indicate a connection with New York City, New York-based Arron Gaines . Research on social engineering forums revealed usernames that were traced back to email addresses with a common web domain which has been separately flagged for malicious activity. Additional research on the emails suggests Gaines and Ellison are the probable end users.'],\n",
       "    'notes': ['Suspected phishing campaign organizer',\n",
       "     'Previous conviction for misuse of IT resources',\n",
       "     'Phishing victims: [[Elsie Graves]], [[Wilfredo Goodman]], Jan Jackson'],\n",
       "    'associates': ['Arron Gaines'],\n",
       "    'email_addresses': ['senftube@mycbt(.)me', 'senftube@gdlcasas(.)com'],\n",
       "    'phone_numbers': ['+1539-514-7562'],\n",
       "    'residence_names': ['Whitley Apartments'],\n",
       "    'residence_addresses': ['301 Brazos St, Austin, TX 78701'],\n",
       "    'residence_coordinates': ['30.265137528652627, -97.74221913760653'],\n",
       "    'residence_maps': ['<br/>! Whitley_Apartments.JPG | 500'],\n",
       "    'work_names': ['Undetermined; near Burnet Road Business Park'],\n",
       "    'work_addresses': ['9705 Burnet Rd, Austin, TX 78758'],\n",
       "    'work_coordinates': ['30.380637466851038, -97.72340578015162'],\n",
       "    'work_maps': ['<br/>! Burnet_Road_Business_Park.JPG | 500']}}}]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entities_path = \"Investigation/People\"\n",
    "assets_path = \"Investigation/Assets\"\n",
    "\n",
    "builder = ProfileBuilder(entities_path, assets_path)\n",
    "out = builder.create_profiles()\n",
    "out\n",
    "\n",
    "# builder.final_entities_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d9968e",
   "metadata": {},
   "source": [
    "- location maps should use similar logic to person pic path to get just the file name (regex)\n",
    "- try re.sub again all tags to get rid of things like <em></em>\n",
    "- need to unpack names from double brackets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "daa4edb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Determine if file is for Person of Interest\n",
    "# to_find = \"Person_Of_Interest\"\n",
    "\n",
    "# is_person_file = any([to_find in tag.get_text() for tag in all_p_tags])\n",
    "# is_person_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "31e7289e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'John Doe': {'soup': 'aaaaaaa', 'parsed': {'pic_path': '123file.jpg'}}}"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Same structure as raw_entities_data\n",
    "test = {\n",
    "    \"John Doe\": {\n",
    "        \"soup\": \"aaaaaaa\",\n",
    "        \"parsed\": {\n",
    "            \"pic_path\": \"123file.jpg\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "type(test)\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "910ccc2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'John Doe': {'pic_path': '123file.jpg'}}]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting a list of dicts with just the parsed info at the end\n",
    "[{k:v[\"parsed\"]} for k,v in test.items()]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
